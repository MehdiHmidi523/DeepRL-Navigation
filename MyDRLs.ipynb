{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "colab": {
      "name": "MyDRLs.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "0fUUZqZE05YG",
        "indM3Vw309G0",
        "9V8eWO4g1XwO"
      ]
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWRButZ94SLX"
      },
      "source": [
        "# DDPG, PPO, SAC under Partial Observability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fUUZqZE05YG"
      },
      "source": [
        "## Kinematic and Lidar Model Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPaI-Ms90p8p"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def SparseDepth(x0, x1, y0, y1):\n",
        "    rec = []\n",
        "    dx = abs(x1 - x0)\n",
        "    dy = abs(y1 - y0)\n",
        "    x, y = x0, y0\n",
        "    sx = -1 if x0 > x1 else 1\n",
        "    sy = -1 if y0 > y1 else 1\n",
        "    if dx > dy:\n",
        "        err = dx / 2.0\n",
        "        while x != x1:\n",
        "            rec.append((x, y))\n",
        "            err -= dy\n",
        "            if err < 0:\n",
        "                y += sy\n",
        "                err += dx\n",
        "            x += sx\n",
        "    else:\n",
        "        err = dy / 2.0\n",
        "        while y != y1:\n",
        "            rec.append((x, y))\n",
        "            err -= dx\n",
        "            if err < 0:\n",
        "                x += sx\n",
        "                err += dy\n",
        "            y += sy\n",
        "    return rec\n",
        "\n",
        "\n",
        "def distance_to_obstacle(robot_position, robot_sight_param, sensor_data):\n",
        "    depth_awareness = []\n",
        "    inter = (robot_sight_param[2] - robot_sight_param[1]) / (robot_sight_param[0] - 1)\n",
        "    for i in range(robot_sight_param[0]):\n",
        "        theta = robot_position[2] + robot_sight_param[1] + i * inter\n",
        "        depth_awareness.append(\n",
        "            [robot_position[0] + sensor_data[i] * np.cos(np.deg2rad(theta)), robot_position[1] + sensor_data[i] * np.sin(np.deg2rad(theta))])\n",
        "    return depth_awareness\n",
        "\n",
        "\n",
        "def gaussian(x, mu, sig):\n",
        "    return 1./(np.sqrt(2.*np.pi)*sig)*np.exp(-np.power((x - mu)/sig, 2.)/2)\n",
        "\n",
        "\n",
        "class Vision:\n",
        "    def __init__(self,\n",
        "                 img_map,\n",
        "                 sensor_size=21,\n",
        "                 start_angle=-90.0,\n",
        "                 end_angle=90.0,\n",
        "                 max_dist=500.0,\n",
        "                 ):\n",
        "        self.sensor_size = sensor_size\n",
        "        self.start_angle = start_angle\n",
        "        self.end_angle = end_angle\n",
        "        self.farthest = max_dist\n",
        "        self.img_map = img_map\n",
        "\n",
        "    def line_of_sight(self, robot_position, theta):\n",
        "        end = np.array(\n",
        "            (robot_position[0] + self.farthest * np.cos(np.deg2rad(theta)), robot_position[1] + self.farthest * np.sin(np.deg2rad(theta))))\n",
        "        x0, y0 = int(robot_position[0]), int(robot_position[1])\n",
        "        x1, y1 = int(end[0]), int(end[1])\n",
        "        plist = SparseDepth(x0, x1, y0, y1)\n",
        "        zone = self.farthest\n",
        "        for p in plist:\n",
        "            if p[1] >= self.img_map.shape[0] or p[0] >= self.img_map.shape[1] or p[1] < 0 or p[0] < 0:\n",
        "                continue\n",
        "            if self.img_map[p[1], p[0]] < 0.5:\n",
        "                aux = np.power(float(p[0]) - robot_position[0], 2) + np.power(float(p[1]) - robot_position[1], 2)\n",
        "                aux = np.sqrt(aux)\n",
        "                if aux < zone:\n",
        "                    zone = aux\n",
        "        return zone\n",
        "\n",
        "    def measure_depth(self, current_pos):\n",
        "        sense_data = []\n",
        "        inter = (self.end_angle - self.start_angle) / (self.sensor_size - 1)\n",
        "        for i in range(self.sensor_size):\n",
        "            theta = current_pos[2] + self.start_angle + i * inter\n",
        "            sense_data.append(self.line_of_sight(np.array((current_pos[0], current_pos[1])), theta))\n",
        "        plist = distance_to_obstacle(current_pos, [self.sensor_size, self.start_angle, self.end_angle], sense_data)\n",
        "        return sense_data, plist\n",
        "\n",
        "class RoboticAssistant:\n",
        "    def __init__(self, v_range=60, w_range=90, d=5, wu=9, wv=4, car_w=9, car_f=7, car_r=10, dt=0.1):\n",
        "        self.x = 0\n",
        "        self.y = 0\n",
        "        self.theta = 0\n",
        "        self.v = 0\n",
        "        self.w = 0\n",
        "        self.record = []\n",
        "        self.v_range = v_range  # Control Constrain\n",
        "        self.w_range = w_range\n",
        "        self.d = d  # Wheel Distance\n",
        "        self.wu = wu  # Wheel size\n",
        "        self.wv = wv\n",
        "        self.car_w = car_w  # Car size\n",
        "        self.car_f = car_f\n",
        "        self.car_r = car_r\n",
        "        self.corps()\n",
        "        self.dt = dt  # Simulation delta time\n",
        "\n",
        "    def update(self):\n",
        "        if self.v > self.v_range:\n",
        "            self.v = self.v_range\n",
        "        elif self.v < -self.v_range:\n",
        "            self.v = -self.v_range\n",
        "        if self.w > self.w_range:\n",
        "            self.w = self.w_range\n",
        "        elif self.w < -self.w_range:\n",
        "            self.w = -self.w_range\n",
        "        self.x += self.v * np.cos(np.deg2rad(self.theta)) * self.dt\n",
        "        self.y += self.v * np.sin(np.deg2rad(self.theta)) * self.dt\n",
        "        self.theta += self.w * self.dt\n",
        "        self.theta = self.theta % 360\n",
        "        self.record.append((self.x, self.y, self.theta))\n",
        "        self.corps()\n",
        "\n",
        "    def redo(self):\n",
        "        self.x -= self.v * np.cos(np.deg2rad(self.theta)) * self.dt\n",
        "        self.y -= self.v * np.sin(np.deg2rad(self.theta)) * self.dt\n",
        "        self.theta -= self.w * self.dt\n",
        "        self.theta = self.theta % 360\n",
        "        self.record.pop()\n",
        "\n",
        "    def control(self, v, w):\n",
        "        self.v = v\n",
        "        self.w = w\n",
        "        if self.v > self.v_range:\n",
        "            self.v = self.v_range\n",
        "        elif self.v < -self.v_range:\n",
        "            self.v = -self.v_range\n",
        "        if self.w > self.w_range:\n",
        "            self.w = self.w_range\n",
        "        elif self.w < -self.w_range:\n",
        "            self.w = -self.w_range\n",
        "\n",
        "    def corps(self):\n",
        "        pts1 = change_direction_vis(self.car_f, self.car_w / 2, -self.theta) + np.array((self.x, self.y))\n",
        "        pts2 = change_direction_vis(self.car_f, -self.car_w / 2, -self.theta) + np.array((self.x, self.y))\n",
        "        pts3 = change_direction_vis(-self.car_r, self.car_w / 2, -self.theta) + np.array((self.x, self.y))\n",
        "        pts4 = change_direction_vis(-self.car_r, -self.car_w / 2, -self.theta) + np.array((self.x, self.y))\n",
        "        self.dimensions = (pts1.astype(int), pts2.astype(int), pts3.astype(int), pts4.astype(int))\n",
        "\n",
        "    def render(self, img=np.ones((600, 600, 3))):\n",
        "        rec_max = 1000\n",
        "        start = 0 if len(self.record) < rec_max else len(self.record) - rec_max\n",
        "\n",
        "        color = (0 / 255, 97 / 255, 255 / 255)\n",
        "        for i in range(start, len(self.record) - 1):\n",
        "            cv2.line(img, (int(self.record[i][0]), int(self.record[i][1])),\n",
        "                     (int(self.record[i + 1][0]), int(self.record[i + 1][1])), color, 1)\n",
        "\n",
        "        pts1, pts2, pts3, pts4 = self.dimensions\n",
        "        color = (0, 0, 0)\n",
        "        size = 1\n",
        "        cv2.line(img, tuple(pts1.astype(np.int).tolist()), tuple(pts2.astype(np.int).tolist()), color, size)\n",
        "        cv2.line(img, tuple(pts1.astype(np.int).tolist()), tuple(pts3.astype(np.int).tolist()), color, size)\n",
        "        cv2.line(img, tuple(pts3.astype(np.int).tolist()), tuple(pts4.astype(np.int).tolist()), color, size)\n",
        "        cv2.line(img, tuple(pts2.astype(np.int).tolist()), tuple(pts4.astype(np.int).tolist()), color, size)\n",
        "\n",
        "        t1 = change_direction_vis(6, 0, -self.theta) + np.array((self.x, self.y))\n",
        "        t2 = change_direction_vis(0, 4, -self.theta) + np.array((self.x, self.y))\n",
        "        t3 = change_direction_vis(0, -4, -self.theta) + np.array((self.x, self.y))\n",
        "        cv2.line(img, (int(self.x), int(self.y)), (int(t1[0]), int(t1[1])), (0, 0, 1), 2)\n",
        "        cv2.line(img, (int(t2[0]), int(t2[1])), (int(t3[0]), int(t3[1])), (1, 0, 0), 2)\n",
        "\n",
        "        w1 = change_direction_vis(0, self.d, -self.theta) + np.array((self.x, self.y))\n",
        "        w2 = change_direction_vis(0, -self.d, -self.theta) + np.array((self.x, self.y))\n",
        "        img = view_unit(img, int(w1[0]), int(w1[1]), self.wu, self.wv, -self.theta)\n",
        "        img = view_unit(img, int(w2[0]), int(w2[1]), self.wu, self.wv, -self.theta)\n",
        "        img = cv2.line(img, tuple(w1.astype(np.int).tolist()), tuple(w2.astype(np.int).tolist()), (0, 0, 0), 1)\n",
        "        return img\n",
        "\n",
        "\n",
        "def change_direction_vis(x, y, phi_):\n",
        "    phi = np.deg2rad(phi_)\n",
        "    return np.array((x * np.cos(phi) + y * np.sin(phi), -x * np.sin(phi) + y * np.cos(phi)))\n",
        "\n",
        "\n",
        "def view_unit(img, x, y, u, v, phi, color=(0, 0, 0), size=1):\n",
        "    edge1 = change_direction_vis(-u / 2, -v / 2, phi) + np.array((x, y))\n",
        "    edge2 = change_direction_vis(u / 2, -v / 2, phi) + np.array((x, y))\n",
        "    edge3 = change_direction_vis(-u / 2, v / 2, phi) + np.array((x, y))\n",
        "    edge4 = change_direction_vis(u / 2, v / 2, phi) + np.array((x, y))\n",
        "    cv2.line(img, tuple(edge1.astype(np.int).tolist()), tuple(edge2.astype(np.int).tolist()), color, size)\n",
        "    cv2.line(img, tuple(edge1.astype(np.int).tolist()), tuple(edge3.astype(np.int).tolist()), color, size)\n",
        "    cv2.line(img, tuple(edge3.astype(np.int).tolist()), tuple(edge4.astype(np.int).tolist()), color, size)\n",
        "    cv2.line(img, tuple(edge2.astype(np.int).tolist()), tuple(edge4.astype(np.int).tolist()), color, size)\n",
        "    return img"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "indM3Vw309G0"
      },
      "source": [
        "## Indoor Reinforcement Learning Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3C37vj2G1Bkg"
      },
      "source": [
        "import os\n",
        "\n",
        "class IndoorDeepRL:\n",
        "    def __init__(self, map_path=\"map.png\"):\n",
        "        # Read Map\n",
        "        self.terra = cv2.flip(cv2.imread(map_path), 0)\n",
        "        self.terra[self.terra > 128] = 255\n",
        "        self.terra[self.terra <= 128] = 0\n",
        "        self.m = np.asarray(self.terra)\n",
        "        self.m = cv2.cvtColor(self.m, cv2.COLOR_RGB2GRAY)\n",
        "        self.m = self.m.astype(float) / 255.\n",
        "        self.terra = self.terra.astype(float) / 255.\n",
        "        self.lmodel = Vision(self.m)\n",
        "\n",
        "    def createInstance(self):\n",
        "        self.robot = RoboticAssistant(d=5, wu=9, wv=4, car_w=9, car_f=7, car_r=10, dt=0.1)\n",
        "        self.robot.x, self.robot.y = self.random_start_travesable()\n",
        "        self.robot.theta = 360 * np.random.random()\n",
        "        self.pos = (self.robot.x, self.robot.y, self.robot.theta)\n",
        "\n",
        "        self.target = self.random_start_travesable()\n",
        "        self.target_euclidian = np.sqrt((self.robot.x - self.target[0]) ** 2 + (self.robot.y - self.target[1]) ** 2)\n",
        "        target_angle = np.arctan2(self.target[1] - self.robot.y, self.target[0] - self.robot.x) - np.deg2rad(self.robot.theta)\n",
        "        target_distance = [self.target_euclidian * np.cos(target_angle), self.target_euclidian * np.sin(target_angle)]\n",
        "\n",
        "        self.sdata, self.plist = self.lmodel.measure_depth(self.pos)\n",
        "        state = self.existance(self.sdata, target_distance)\n",
        "        return state\n",
        "\n",
        "    def step(self, action):\n",
        "        self.robot.control((action[0] + 1) / 2 * self.robot.v_range, action[1] * self.robot.w_range)\n",
        "        self.robot.update()\n",
        "        # Check for Collision\n",
        "        e1,e2,e3,e4 = self.robot.dimensions\n",
        "        ee1 = SparseDepth(e1[0], e2[0], e1[1], e2[1])\n",
        "        ee2 = SparseDepth(e1[0], e3[0], e1[1], e3[1])\n",
        "        ee3 = SparseDepth(e3[0], e4[0], e3[1], e4[1])\n",
        "        ee4 = SparseDepth(e4[0], e2[0], e4[1], e2[1])\n",
        "        check = ee1+ee2+ee3+ee4\n",
        "        collision = False\n",
        "        for points in check:\n",
        "            if self.m[int(points[1]),int(points[0])]<0.5:\n",
        "                collision = True\n",
        "                self.robot.redo()\n",
        "                self.robot.v = -0.5 * self.robot.v\n",
        "                break\n",
        "        self.pos = (self.robot.x, self.robot.y, self.robot.theta)\n",
        "        self.sdata, self.plist = self.lmodel.measure_depth(self.pos)\n",
        "\n",
        "        # Distance Reward\n",
        "        curr_target_dist = np.sqrt((self.robot.x - self.target[0]) ** 2 + (self.robot.y - self.target[1]) ** 2)\n",
        "        reward_dist = self.target_euclidian - curr_target_dist\n",
        "        # Orientation Reward\n",
        "        orien = np.rad2deg(np.arctan2(self.target[1] - self.robot.y, self.target[0] - self.robot.x))\n",
        "        orientation_error = (orien - self.robot.theta) % 360\n",
        "        if orientation_error > 180:\n",
        "            orientation_error = 360 - orientation_error\n",
        "        reward_orien = np.deg2rad(orientation_error)\n",
        "        # Action Panelty\n",
        "        reward_act = 0.05 if action[0] < -0.5 else 0\n",
        "        # Total\n",
        "        w1 = 1\n",
        "        w2 = 1\n",
        "        w3 = 0.6\n",
        "        reward = w1*reward_dist - w2*reward_orien - w3*reward_act\n",
        "        \n",
        "        # Terminal State \n",
        "        terminated = False\n",
        "        if collision:\n",
        "            reward = -15\n",
        "            terminated = True\n",
        "        if curr_target_dist < 20:\n",
        "            reward = 20\n",
        "            terminated = True\n",
        "\n",
        "        # Relative Coordinate of Target\n",
        "        self.target_euclidian = curr_target_dist\n",
        "        target_angle = np.arctan2(self.target[1] - self.robot.y, self.target[0] - self.robot.x) - np.deg2rad(self.robot.theta)\n",
        "        target_distance = [self.target_euclidian * np.cos(target_angle), self.target_euclidian * np.sin(target_angle)]\n",
        "        state_next = self.existance(self.sdata, target_distance)\n",
        "        return state_next, reward, terminated\n",
        "    \n",
        "    def render(self, gui=True):\n",
        "        experiment_space = self.terra.copy()\n",
        "        for pts in self.plist:\n",
        "            cv2.line(\n",
        "                experiment_space,\n",
        "                (int(1*self.pos[0]), int(1*self.pos[1])), \n",
        "                (int(1*pts[0]), int(1*pts[1])),\n",
        "                (0.0,1.0,0.0), 1)\n",
        "\n",
        "        cv2.circle(experiment_space, (int(1*self.target[0]), int(1*self.target[1])), 10, (1.0,0.5,0.7), 3)\n",
        "        experiment_space = self.robot.render(experiment_space)\n",
        "        experiment_space = cv2.flip(experiment_space,0)\n",
        "        if gui:\n",
        "            cv2.imshow(\"Mapless Navigation\",experiment_space)\n",
        "            k = cv2.waitKey(1)\n",
        "        return experiment_space.copy()\n",
        "    \n",
        "    def random_start_travesable(self):\n",
        "        im_h, im_w = self.m.shape[0], self.m.shape[1]\n",
        "        tx = np.random.randint(0,im_w)\n",
        "        ty = np.random.randint(0,im_h)\n",
        "\n",
        "        kernel = np.ones((10,10),np.uint8)  \n",
        "        m_dilate = 1-cv2.dilate(1-self.m, kernel, iterations=3)\n",
        "        while(m_dilate[ty, tx] < 0.5):\n",
        "            tx = np.random.randint(0,im_w)\n",
        "            ty = np.random.randint(0,im_h)\n",
        "        return tx, ty\n",
        "    \n",
        "    def existance(self, sensor, target):\n",
        "        state_s = [s/200 for s in sensor]\n",
        "        state_t = [t/500 for t in target]\n",
        "        return state_s + state_t\n",
        "\n",
        "#%% md\n",
        "\n",
        "## Trainning and Path configurations\n",
        "\n",
        "#%%\n",
        "\n",
        "training = True\n",
        "render = False\n",
        "load_model = False\n",
        "terrain = \"map.png\"\n",
        "gif_path = \"out/\"\n",
        "model_path = \"save/\"\n",
        "if not os.path.exists(model_path):\n",
        "    os.makedirs(model_path)\n",
        "\n",
        "#%% md\n",
        "\n",
        "## Episode Visualization \n",
        "\n",
        "#%%\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "def visualize(agent, total_eps=2, message=False, render=False, map_path=\"map.png\", gif_path=\"out/\", gif_name=\"test.gif\"):\n",
        "    if not os.path.exists(gif_path):\n",
        "        os.makedirs(gif_path)\n",
        "    images = []\n",
        "\n",
        "    env = IndoorDeepRL(map_path=terrain)\n",
        "    for eps in range(total_eps):\n",
        "        step = 0\n",
        "        max_success_rate = 0\n",
        "        success_count = 0\n",
        "\n",
        "        state = env.createInstance()\n",
        "        r_eps = []\n",
        "        acc_reward = 0.\n",
        "        while(True):\n",
        "            action = agent.choose_action(state, eval=True)\n",
        "            state_next, reward, terminated = env.step(action)\n",
        "            im = env.render(gui=render)\n",
        "            im_pil = Image.fromarray(cv2.cvtColor(np.uint8(im*255),cv2.COLOR_BGR2RGB))\n",
        "            images.append(im_pil)\n",
        "            r_eps.append(reward)\n",
        "            acc_reward += reward\n",
        "            \n",
        "            if message:\n",
        "                print('\\rEps: {:2d}| Step: {:4d} | action:{:+.2f}| R:{:+.2f}| Reps:{:.2f}  '\\\n",
        "                        .format(eps, step, action[0], reward, acc_reward), end='')\n",
        "            \n",
        "            state = state_next.copy()\n",
        "            step += 1\n",
        "            if terminated or step>200:\n",
        "                if message:\n",
        "                    print()\n",
        "                break\n",
        "\n",
        "    print(\"Create GIF ...\")\n",
        "    if gif_path is not None:\n",
        "        images[0].save(gif_path+gif_name, save_all=True, append_images=images[1:], optimize=True, duration=40, loop=0)\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9V8eWO4g1XwO"
      },
      "source": [
        "## Deep RL Algorithms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdqrkupz1cEW"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class DDPG():\n",
        "    def __init__(\n",
        "            self,\n",
        "            model,\n",
        "            learning_rate=[1e-4, 2e-4],\n",
        "            reward_decay=0.98,\n",
        "            memory_size=5000,\n",
        "            batch_size=64,\n",
        "            tau=0.005,\n",
        "            epsilon_params=[1.0, 0.5, 0.00001],\n",
        "            criterion=nn.MSELoss()\n",
        "    ):\n",
        "        self.memory_size = memory_size\n",
        "        self.batch_size = batch_size\n",
        "        self.tau = tau\n",
        "        self.memory = {\"s\": [], \"a\": [], \"r\": [], \"sn\": [], \"end\": []}\n",
        "        self.memory_counter = 0\n",
        "        self.lr = learning_rate\n",
        "        self.gamma = reward_decay\n",
        "        self.criterion = criterion\n",
        "        self.epsilon_params = epsilon_params\n",
        "        self.epsilon = self.epsilon_params[0]\n",
        "        self._build_net(model[0], model[1])\n",
        "\n",
        "    def _build_net(self, anet, cnet):\n",
        "        self.actor = anet()\n",
        "        self.actor_optim = optim.Adam(self.actor.parameters(), lr=self.lr[0])\n",
        "        self.critic = cnet()\n",
        "        self.critic_optim = optim.Adam(self.critic.parameters(), lr=self.lr[1])\n",
        "        self.critic_target = cnet()\n",
        "        self.critic_target.eval()\n",
        "\n",
        "    def save_load_model(self, op, path):\n",
        "        anet_path = path + \"ddpg_anet.pt\"\n",
        "        cnet_path = path + \"ddpg_cnet.pt\"\n",
        "        if op == \"save\":\n",
        "            torch.save(self.critic.state_dict(), cnet_path)\n",
        "            torch.save(self.actor.state_dict(), anet_path)\n",
        "        elif op == \"load\":\n",
        "            self.critic.load_state_dict(torch.load(cnet_path))\n",
        "            self.critic_target.load_state_dict(torch.load(cnet_path))\n",
        "            self.actor.load_state_dict(torch.load(anet_path))\n",
        "\n",
        "    def choose_action(self, s, eval=False):\n",
        "        s_ts = torch.FloatTensor(np.expand_dims(s, 0))\n",
        "        action = self.actor(s_ts)\n",
        "        action = action.cpu().detach().numpy()[0]\n",
        "        if not eval:\n",
        "            action += np.random.normal(0, self.epsilon, action.shape)\n",
        "        else:\n",
        "            action += np.random.normal(0, self.epsilon_params[1], action.shape)\n",
        "\n",
        "        action = np.clip(action, -1, 1)\n",
        "        return action\n",
        "\n",
        "    def store_transition(self, s, a, r, sn, end):\n",
        "        if self.memory_counter <= self.memory_size:\n",
        "            self.memory[\"s\"].append(s)\n",
        "            self.memory[\"a\"].append(a)\n",
        "            self.memory[\"r\"].append(r)\n",
        "            self.memory[\"sn\"].append(sn)\n",
        "            self.memory[\"end\"].append(end)\n",
        "        else:\n",
        "            index = self.memory_counter % self.memory_size\n",
        "            self.memory[\"s\"][index] = s\n",
        "            self.memory[\"a\"][index] = a\n",
        "            self.memory[\"r\"][index] = r\n",
        "            self.memory[\"sn\"][index] = sn\n",
        "            self.memory[\"end\"][index] = end\n",
        "\n",
        "        self.memory_counter += 1\n",
        "\n",
        "    def soft_update(self):\n",
        "        # Store sample to replay buffer\n",
        "        with torch.no_grad():\n",
        "            for targetParam, evalParam in zip(self.critic_target.parameters(), self.critic.parameters()):\n",
        "                targetParam.copy_((1 - self.tau) * targetParam.data + self.tau * evalParam.data)\n",
        "\n",
        "    def learn(self):\n",
        "        # Sample\n",
        "        if self.memory_counter > self.memory_size:\n",
        "            sample_index = np.random.choice(self.memory_size, size=self.batch_size)\n",
        "        else:\n",
        "            sample_index = np.random.choice(self.memory_counter, size=self.batch_size)\n",
        "\n",
        "        s_batch = [self.memory[\"s\"][index] for index in sample_index]\n",
        "        a_batch = [self.memory[\"a\"][index] for index in sample_index]\n",
        "        r_batch = [self.memory[\"r\"][index] for index in sample_index]\n",
        "        sn_batch = [self.memory[\"sn\"][index] for index in sample_index]\n",
        "        end_batch = [self.memory[\"end\"][index] for index in sample_index]\n",
        "\n",
        "        # Construct torch tensor\n",
        "        s_ts = torch.FloatTensor(np.array(s_batch))\n",
        "        a_ts = torch.FloatTensor(np.array(a_batch))\n",
        "        r_ts = torch.FloatTensor(np.array(r_batch))\n",
        "        sn_ts = torch.FloatTensor(np.array(sn_batch))\n",
        "        end_ts = torch.FloatTensor(np.array(end_batch))\n",
        "\n",
        "        # TD-target\n",
        "        with torch.no_grad():\n",
        "            a_next = self.actor(sn_ts)\n",
        "            q_next_target = self.critic_target(sn_ts, a_next)\n",
        "            q_target = r_ts + end_ts * self.gamma * q_next_target\n",
        "\n",
        "        # Critic loss\n",
        "        q_eval = self.critic(s_ts, a_ts)\n",
        "        self.critic_loss = self.criterion(q_eval, q_target)\n",
        "\n",
        "        self.critic_optim.zero_grad()\n",
        "        self.critic_loss.backward()\n",
        "        self.critic_optim.step()\n",
        "\n",
        "        # Actor loss\n",
        "        a_curr = self.actor(s_ts)\n",
        "        q_current = self.critic(s_ts, a_curr)\n",
        "        self.actor_loss = -q_current.mean()\n",
        "\n",
        "        self.actor_optim.zero_grad()\n",
        "        self.actor_loss.backward()\n",
        "        self.actor_optim.step()\n",
        "\n",
        "        # Update target network and noise variance\n",
        "        self.soft_update()\n",
        "        if self.epsilon > self.epsilon_params[1]:\n",
        "            self.epsilon -= self.epsilon_params[2]\n",
        "        else:\n",
        "            self.epsilon = self.epsilon_params[1]\n",
        "\n",
        "        return float(self.actor_loss.detach().cpu().numpy()), float(self.critic_loss.detach().cpu().numpy())\n",
        "\n",
        "class SAC():\n",
        "    def __init__(\n",
        "        self,\n",
        "        model,\n",
        "        n_actions,\n",
        "        learning_rate = [1e-4, 2e-4],\n",
        "        reward_decay = 0.98,\n",
        "        replace_target_iter = 300,\n",
        "        memory_size = 5000,\n",
        "        batch_size = 64,\n",
        "        tau = 0.01,\n",
        "        alpha = 0.5,\n",
        "        auto_entropy_tuning = True,\n",
        "        criterion = nn.MSELoss()\n",
        "    ):\n",
        "        # initialize parameters\n",
        "        self.n_actions = n_actions\n",
        "        self.lr = learning_rate\n",
        "        self.gamma = reward_decay\n",
        "        self.memory_size = memory_size\n",
        "        self.batch_size = batch_size\n",
        "        self.tau = tau\n",
        "        self.alpha = alpha\n",
        "        self.auto_entropy_tuning = auto_entropy_tuning\n",
        "        self.criterion = criterion\n",
        "        self._build_net(model[0], model[1])\n",
        "        self.init_memory()\n",
        "\n",
        "    def _build_net(self, anet, cnet):\n",
        "        # Policy Network\n",
        "        self.actor = anet()\n",
        "        self.actor_optim = optim.Adam(self.actor.parameters(), lr=self.lr[0])\n",
        "        # Evaluation Critic Network (new)\n",
        "        self.critic = cnet()\n",
        "        self.critic_optim = optim.Adam(self.critic.parameters(), lr=self.lr[1])\n",
        "        # Target Critic Network (old)\n",
        "        self.critic_target = cnet()\n",
        "        self.critic_target.eval()\n",
        "\n",
        "        if self.auto_entropy_tuning == True:\n",
        "            self.target_entropy = -torch.Tensor(self.n_actions)\n",
        "            self.log_alpha = torch.zeros(1, requires_grad=True)\n",
        "            self.alpha_optim = optim.Adam([self.log_alpha], lr=0.0001)\n",
        "    \n",
        "    def save_load_model(self, op, path):\n",
        "        anet_path = path + \"sac_anet.pt\"\n",
        "        cnet_path = path + \"sac_cnet.pt\"\n",
        "        if op == \"save\":\n",
        "            torch.save(self.critic.state_dict(), cnet_path)\n",
        "            torch.save(self.actor.state_dict(), anet_path)\n",
        "        elif op == \"load\":\n",
        "            self.critic.load_state_dict(torch.load(cnet_path))\n",
        "            self.critic_target.load_state_dict(torch.load(cnet_path))\n",
        "            self.actor.load_state_dict(torch.load(anet_path))\n",
        "\n",
        "    def choose_action(self, s, eval=False):\n",
        "        s_ts = torch.FloatTensor(np.expand_dims(s,0))\n",
        "        if eval == False:\n",
        "            action, _, _ = self.actor.sample(s_ts)\n",
        "        else:\n",
        "            _, _, action = self.actor.sample(s_ts)\n",
        "        \n",
        "        action = action.cpu().detach().numpy()[0]\n",
        "        return action\n",
        "\n",
        "    def init_memory(self):\n",
        "        self.memory_counter = 0\n",
        "        self.memory = {\"s\":[], \"a\":[], \"r\":[], \"sn\":[], \"end\":[]}\n",
        "\n",
        "    def store_transition(self, s, a, r, sn, end):\n",
        "        if self.memory_counter <= self.memory_size:\n",
        "            self.memory[\"s\"].append(s)\n",
        "            self.memory[\"a\"].append(a)\n",
        "            self.memory[\"r\"].append(r)\n",
        "            self.memory[\"sn\"].append(sn)\n",
        "            self.memory[\"end\"].append(end)\n",
        "        else:\n",
        "            index = self.memory_counter % self.memory_size\n",
        "            self.memory[\"s\"][index] = s\n",
        "            self.memory[\"a\"][index] = a\n",
        "            self.memory[\"r\"][index] = r\n",
        "            self.memory[\"sn\"][index] = sn\n",
        "            self.memory[\"end\"][index] = end\n",
        "\n",
        "        self.memory_counter += 1\n",
        "\n",
        "    def soft_update(self, TAU=0.01):\n",
        "        with torch.no_grad():\n",
        "            for targetParam, evalParam in zip(self.critic_target.parameters(), self.critic.parameters()):\n",
        "                targetParam.copy_((1 - self.tau)*targetParam.data + self.tau*evalParam.data)\n",
        "\n",
        "    def learn(self):\n",
        "        # sample batch memory from all memory\n",
        "        if self.memory_counter > self.memory_size:\n",
        "            sample_index = np.random.choice(self.memory_size, size=self.batch_size)\n",
        "        else:\n",
        "            sample_index = np.random.choice(self.memory_counter, size=self.batch_size)\n",
        "        \n",
        "        s_batch = [self.memory[\"s\"][index] for index in sample_index]\n",
        "        a_batch = [self.memory[\"a\"][index] for index in sample_index]\n",
        "        r_batch = [self.memory[\"r\"][index] for index in sample_index]\n",
        "        sn_batch = [self.memory[\"sn\"][index] for index in sample_index]\n",
        "        end_batch = [self.memory[\"end\"][index] for index in sample_index]\n",
        "\n",
        "        # Construct torch tensor\n",
        "        s_ts = torch.FloatTensor(np.array(s_batch))\n",
        "        a_ts = torch.FloatTensor(np.array(a_batch))\n",
        "        r_ts = torch.FloatTensor(np.array(r_batch))\n",
        "        sn_ts = torch.FloatTensor(np.array(sn_batch))\n",
        "        end_ts = torch.FloatTensor(np.array(end_batch))\n",
        "        \n",
        "        # TD-target\n",
        "        with torch.no_grad():\n",
        "            a_next, logpi_next, _ = self.actor.sample(sn_ts)\n",
        "            q_next_target = self.critic_target(sn_ts, a_next) - self.alpha * logpi_next\n",
        "            q_target = r_ts + end_ts * self.gamma * q_next_target\n",
        "        \n",
        "        # Critic loss\n",
        "        q_eval = self.critic(s_ts, a_ts)\n",
        "        self.critic_loss = self.criterion(q_eval, q_target)\n",
        "\n",
        "        self.critic_optim.zero_grad()\n",
        "        self.critic_loss.backward()\n",
        "        self.critic_optim.step()\n",
        "\n",
        "        # Actor loss\n",
        "        a_curr, logpi_curr, _ = self.actor.sample(s_ts)\n",
        "        q_current = self.critic(s_ts, a_curr)\n",
        "        self.actor_loss = ((self.alpha*logpi_curr) - q_current).mean()\n",
        "\n",
        "        self.actor_optim.zero_grad()\n",
        "        self.actor_loss.backward()\n",
        "        self.actor_optim.step()\n",
        "\n",
        "        self.soft_update()\n",
        "        \n",
        "        # Adaptive entropy adjustment\n",
        "        if self.auto_entropy_tuning:\n",
        "            alpha_loss = -(self.log_alpha * (logpi_curr + self.target_entropy).detach()).mean()\n",
        "            self.alpha_optim.zero_grad()\n",
        "            alpha_loss.backward()\n",
        "            self.alpha_optim.step()\n",
        "            self.alpha = float(self.log_alpha.exp().detach().cpu().numpy())\n",
        "        \n",
        "        return float(self.actor_loss.detach().cpu().numpy()), float(self.critic_loss.detach().cpu().numpy())\n",
        "\n",
        "class PPO():\n",
        "    def __init__(\n",
        "        self,\n",
        "        model,\n",
        "        learning_rate = [1e-4, 2e-4],\n",
        "        reward_decay = 0.99,\n",
        "        batch_size = 2000,\n",
        "        eps_clip = 0.2\n",
        "    ):\n",
        "        self.lr = learning_rate\n",
        "        self.gamma = reward_decay\n",
        "        self.batch_size = batch_size\n",
        "        self.eps_clip = eps_clip\n",
        "        self._build_net(model[0], model[1])\n",
        "        self.init_memory()\n",
        "    \n",
        "    def _build_net(self, anet, cnet):\n",
        "        # Policy Network\n",
        "        self.actor = anet()\n",
        "        self.actor_optim = optim.Adam(self.actor.parameters(), lr=self.lr[0])\n",
        "        # Critic Network \n",
        "        self.critic = cnet()\n",
        "        self.critic_optim = optim.Adam(self.critic.parameters(), lr=self.lr[1])\n",
        "    \n",
        "    def save_load_model(self, op, path):\n",
        "        anet_path = path + \"ppo_anet.pt\"\n",
        "        cnet_path = path + \"ppo_cnet.pt\"\n",
        "        if op == \"save\":\n",
        "            torch.save(self.actor.state_dict(), anet_path)\n",
        "            torch.save(self.critic.state_dict(), cnet_path)\n",
        "        elif op == \"load\":\n",
        "            self.actor.load_state_dict(torch.load(anet_path))\n",
        "            self.critic.load_state_dict(torch.load(cnet_path))\n",
        "            \n",
        "    def choose_action(self, s, eval=False):\n",
        "        s_ts = torch.FloatTensor(np.expand_dims(s,0))\n",
        "        logp = None\n",
        "        if eval == False:\n",
        "            a_ts, logp_ts = self.actor.sample(s_ts)\n",
        "            a_ts = torch.clamp(a_ts, min=-1, max=1)\n",
        "            action = a_ts.cpu().detach().numpy()[0]\n",
        "            logp = logp_ts.cpu().detach().numpy()[0]\n",
        "            return action, logp\n",
        "        else:\n",
        "            a_ts, logp_ts = self.actor.sample(s_ts)\n",
        "            a_ts = torch.clamp(a_ts, min=-1, max=1)\n",
        "            action = a_ts.cpu().detach().numpy()[0]\n",
        "            return action\n",
        "    \n",
        "    def init_memory(self):\n",
        "        self.memory_counter = 0\n",
        "        self.memory = {\"s\":[], \"a\":[], \"r\":[], \"sn\":[], \"end\":[], \"logp\":[], \"return\":[]}\n",
        "        \n",
        "    def store_transition(self, s, a, r, sn, end, logp):\n",
        "        self.memory[\"s\"].append(s)\n",
        "        self.memory[\"a\"].append(a)\n",
        "        self.memory[\"r\"].append(r)\n",
        "        self.memory[\"sn\"].append(sn)\n",
        "        self.memory[\"end\"].append(end)\n",
        "        self.memory[\"logp\"].append(logp)\n",
        "        self.memory_counter += 1\n",
        "\n",
        "    def run_return(self):\n",
        "        self.memory[\"return\"] = []\n",
        "        discounted_reward = 0\n",
        "        for reward, end in zip(reversed(self.memory[\"r\"]), reversed(self.memory[\"end\"])):\n",
        "            if end == 0:\n",
        "                discounted_reward = reward\n",
        "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
        "            self.memory[\"return\"].insert(0, discounted_reward)\n",
        "\n",
        "    def learn(self, iter):\n",
        "        self.run_return()\n",
        "        # Construct torch tensor\n",
        "        s_ts = torch.FloatTensor(np.array(self.memory[\"s\"]))\n",
        "        a_ts = torch.FloatTensor(np.array(self.memory[\"a\"]))\n",
        "        r_ts = torch.FloatTensor(np.expand_dims(np.array(self.memory[\"r\"]), 1))\n",
        "        sn_ts = torch.FloatTensor(np.array(self.memory[\"sn\"]))\n",
        "        end_ts = torch.FloatTensor(np.expand_dims(np.array(self.memory[\"end\"]), 1))\n",
        "\n",
        "        logp_ts = torch.FloatTensor(np.expand_dims(np.array(self.memory[\"logp\"]), 1))\n",
        "        return_ts = torch.FloatTensor(np.expand_dims(np.array(self.memory[\"return\"]), 1))\n",
        "        return_ts = (return_ts - return_ts.mean()) / (return_ts.std() + 1e-5)\n",
        "\n",
        "        for it in range(1):\n",
        "            # Evaluate policy and state-value\n",
        "            dist = self.actor.distribution(s_ts)\n",
        "            logp_curr = dist.log_prob(a_ts)\n",
        "            ent   = dist.entropy()\n",
        "            value = self.critic(s_ts)\n",
        "\n",
        "            # Compute loss\n",
        "            ratio = (logp_curr - logp_ts.detach()).exp()\n",
        "            advantage = return_ts - value.detach()\n",
        "            surr1 = advantage * ratio\n",
        "            surr2 = advantage * torch.clamp(ratio, 1-self.eps_clip, 1+self.eps_clip)\n",
        "            pg_loss = (-advantage*logp_curr).mean() #-torch.min(surr1, surr2).mean()\n",
        "            v_loss = torch.nn.MSELoss()(value, return_ts).mean()\n",
        "            ent_loss = ent.mean()\n",
        "            loss = pg_loss + 0.5*v_loss - 0.01*ent_loss\n",
        "            \n",
        "            # Optimize parameters\n",
        "            self.critic_optim.zero_grad()\n",
        "            self.actor_optim.zero_grad()\n",
        "            loss.backward()\n",
        "            self.critic_optim.step()\n",
        "            self.actor_optim.step()\n",
        "            if it%10 == 0:\n",
        "                print(  \"Iter\", it, \\\n",
        "                        \", pg_loss:\", pg_loss.detach().cpu().numpy(), \\\n",
        "                        \", ent_loss:\", ent_loss.detach().cpu().numpy(), \\\n",
        "                        \", v_loss:\", v_loss.detach().cpu().numpy())\n",
        "\n",
        "        self.init_memory()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKB4fNbM11HS"
      },
      "source": [
        "##DDPG Agent training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "zn4mBduJ15Gr",
        "outputId": "3a862910-6ce9-4c3c-862f-8b1a583bd457"
      },
      "source": [
        "batch_size = 64\n",
        "eval_eps = 50\n",
        "LOG_SIG_MAX = 2\n",
        "LOG_SIG_MIN = -20\n",
        "epsilon = 1e-6\n",
        "\n",
        "class PolicyNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PolicyNet, self).__init__()\n",
        "        self.layer1 = nn.Linear(23, 512)\n",
        "        self.layer2 = nn.Linear(512, 512)\n",
        "        self.layer3 = nn.Linear(512, 512)\n",
        "        self.layer4 = nn.Linear(512, 2)\n",
        "\n",
        "    def forward(self, s):\n",
        "        h_fc1 = F.relu(self.layer1(s))\n",
        "        h_fc2 = F.relu(self.layer2(h_fc1))\n",
        "        h_fc3 = F.relu(self.layer3(h_fc2))\n",
        "        mu = torch.tanh(self.layer4(h_fc3))\n",
        "        return mu\n",
        "\n",
        "class QNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(QNet, self).__init__()\n",
        "        self.layer1 = nn.Linear(23, 512)\n",
        "        self.layer2 = nn.Linear(512+2, 512)\n",
        "        self.layer3 = nn.Linear(512, 512)\n",
        "        self.layer4 = nn.Linear(512, 1)\n",
        "    \n",
        "    def forward(self, s, a):\n",
        "        h_fc1 = F.relu(self.layer1(s))\n",
        "        h_fc1_a = torch.cat((h_fc1, a), 1)\n",
        "        h_fc2 = F.relu(self.layer2(h_fc1_a))\n",
        "        h_fc3 = F.relu(self.layer3(h_fc2))\n",
        "        q_out = self.layer4(h_fc3)\n",
        "        return q_out\n",
        "\n",
        "agent_mind_ddpg = DDPG(\n",
        "    model = [PolicyNet, QNet],\n",
        "    learning_rate = [0.0001, 0.0001],\n",
        "    reward_decay = 0.99,\n",
        "    memory_size = 10000,\n",
        "    batch_size = batch_size)\n",
        "\n",
        "if load_model:\n",
        "    print(\"Load model ...\", model_path)\n",
        "    agent_mind_ddpg.save_load_model(\"load\", model_path)\n",
        "\n",
        "env = IndoorDeepRL(map_path=terrain)\n",
        "total_steps = 0\n",
        "best_rate = 0\n",
        "success_count = 0\n",
        "\n",
        "print(\"eps, step, total_steps, action[0], reward, loss_a, loss_c, agent_mind_ddpg.epsilon, acc_reward/step\")\n",
        "for eps in range(4500): # how many episode do you want?\n",
        "    state = env.createInstance()\n",
        "    step = 0\n",
        "    loss_a = loss_c = 0.\n",
        "    acc_reward = 0.\n",
        "    while(True):\n",
        "        # Choose action and run\n",
        "        if training:\n",
        "            action = agent_mind_ddpg.choose_action(state, eval=False)\n",
        "        else:\n",
        "            action = agent_mind_ddpg.choose_action(state, eval=True)\n",
        "        state_next, reward, terminated = env.step(action)\n",
        "        end = 0 if terminated else 1\n",
        "        agent_mind_ddpg.store_transition(state, action, reward, state_next, end)\n",
        "\n",
        "        # Render environment\n",
        "        im = env.render(gui=render)\n",
        "\n",
        "        # Learn the model\n",
        "        loss_a = loss_c = 0.\n",
        "        if total_steps > batch_size and training:\n",
        "            loss_a, loss_c = agent_mind_ddpg.learn()\n",
        "        step += 1\n",
        "        total_steps += 1\n",
        "        # Print information\n",
        "        acc_reward += reward\n",
        "        print('\\r{:3d} ; {:4d}; {:6d}; {:+.2f}; {:+.2f}; {:+.2f}; {:+.2f}; {:.3f}; {:.2f}'\n",
        "              .format(eps, step, total_steps, action[0], reward, loss_a, loss_c, agent_mind_ddpg.epsilon, acc_reward/step), end='')\n",
        "        state = state_next.copy()\n",
        "        if terminated or step>200:\n",
        "            # Count the successful times\n",
        "            if reward > 2:\n",
        "                success_count += 1\n",
        "            print()\n",
        "            break\n",
        "\n",
        "    if eps>0 and eps%eval_eps==0:\n",
        "        # Sucess rate\n",
        "        success_rate = success_count / eval_eps\n",
        "        success_count = 0\n",
        "        # Save the best model\n",
        "        if success_rate >= best_rate:\n",
        "            best_rate = success_rate\n",
        "            if training:\n",
        "                print(\"Save model to \" + model_path)\n",
        "                agent_mind_ddpg.save_load_model(\"save\", model_path)\n",
        "        print(\"Success Rate (current/max):\", success_rate, \"/\", best_rate)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-99909fc28635>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0magent_mind_ddpg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_load_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"load\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIndoorDeepRL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mterrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0mtotal_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0mbest_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-a231e803c09d>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, map_path)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;31m# Read Map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterra\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterra\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterra\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterra\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterra\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterra\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: '>' not supported between instances of 'NoneType' and 'int'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pratA_WB1-rf"
      },
      "source": [
        "visualize(agent_mind_ddpg, total_eps=4, map_path=terrain, gif_path=gif_path, gif_name=\"ddpg_\"+str(eps).zfill(4)+\".gif\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kD3RLZvk2ZV4"
      },
      "source": [
        "## SAC Training\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H82WG74r2bsK"
      },
      "source": [
        "batch_size = 64\n",
        "eval_eps = 50\n",
        "LOG_SIG_MAX = 2\n",
        "LOG_SIG_MIN = -20\n",
        "epsilon = 1e-6\n",
        "\n",
        "from torch.distributions import Normal\n",
        "\n",
        "class PolicyNetGaussian(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PolicyNetGaussian, self).__init__()\n",
        "        self.layer1 = nn.Linear(23, 512)\n",
        "        self.layer2 = nn.Linear(512, 512)\n",
        "        self.layer3 = nn.Linear(512, 512)\n",
        "        self.fc4_mean = nn.Linear(512, 2)\n",
        "        self.fc4_logstd = nn.Linear(512, 2)\n",
        "\n",
        "    def forward(self, s):\n",
        "        h_fc1 = F.relu(self.layer1(s))\n",
        "        h_fc2 = F.relu(self.layer2(h_fc1))\n",
        "        h_fc3 = F.relu(self.layer3(h_fc2))\n",
        "        a_mean = self.fc4_mean(h_fc3)\n",
        "        a_logstd = self.fc4_logstd(h_fc3)\n",
        "        a_logstd = torch.clamp(a_logstd, min=LOG_SIG_MIN, max=LOG_SIG_MAX)\n",
        "        return a_mean, a_logstd\n",
        "    \n",
        "    def sample(self, s):\n",
        "        a_mean, a_logstd = self.forward(s)\n",
        "        a_std = a_logstd.exp()\n",
        "        normal = Normal(a_mean, a_std)\n",
        "        x_t = normal.rsample()\n",
        "        action = torch.tanh(x_t)\n",
        "        log_prob = normal.log_prob(x_t)\n",
        "\n",
        "        # Enforcing action Bound\n",
        "        log_prob -= torch.log(1 - action.pow(2) + epsilon)\n",
        "        log_prob = log_prob.sum(1, keepdim=True)\n",
        "        return action, log_prob, torch.tanh(a_mean)\n",
        "\n",
        "agent_mind_sac = SAC(\n",
        "    model = [PolicyNetGaussian, QNet],\n",
        "    n_actions = 2,\n",
        "    learning_rate = [0.0001, 0.0001],\n",
        "    reward_decay = 0.99,\n",
        "    memory_size = 10000,\n",
        "    batch_size = batch_size,\n",
        "    alpha = 0.1,\n",
        "    auto_entropy_tuning=True)\n",
        "\n",
        "if load_model:\n",
        "    print(\"Load model ...\", model_path)\n",
        "    agent_mind_sac.save_load_model(\"load\", model_path)\n",
        "\n",
        "env = IndoorDeepRL(map_path=terrain)\n",
        "total_step = 0\n",
        "max_success_rate = 0\n",
        "success_count = 0\n",
        "\n",
        "print(\"eps, step, total_step, action[0], reward, loss_a, loss_c, agent_mind_sac.alpha, acc_reward/step\")\n",
        "for eps in range(4500):\n",
        "    state = env.createInstance()\n",
        "    step = 0\n",
        "    loss_a = loss_c = 0.\n",
        "    acc_reward = 0.\n",
        "\n",
        "    while(True):\n",
        "        # Choose action and run\n",
        "        if training:\n",
        "            action = agent_mind_sac.choose_action(state, eval=False)\n",
        "        else:\n",
        "            action = agent_mind_sac.choose_action(state, eval=True)\n",
        "        state_next, reward, terminated = env.step(action)\n",
        "        end = 0 if terminated else 1\n",
        "        agent_mind_sac.store_transition(state, action, reward, state_next, end)\n",
        "\n",
        "        # Render environment\n",
        "        im = env.render(gui=render)\n",
        "\n",
        "        # Learn the model\n",
        "        loss_a = loss_c = 0.\n",
        "        if total_step > batch_size and training:\n",
        "            loss_a, loss_c = agent_mind_sac.learn()\n",
        "        step += 1\n",
        "        total_step += 1\n",
        "\n",
        "        # Print information\n",
        "        acc_reward += reward\n",
        "        print('\\r {:3d}; {:4d}; {:6d}; {:+.2f}; {:+.2f}; {:+.2f}; {:+.2f};  {:.3f}; {:.2f}  '\\\n",
        "                .format(eps, step, total_step, action[0], reward, loss_a, loss_c, agent_mind_sac.alpha, acc_reward/step), end='')\n",
        "\n",
        "        state = state_next.copy()\n",
        "        if terminated or step>200:\n",
        "            # Count the successful times\n",
        "            if reward > 5:\n",
        "                success_count += 1\n",
        "            print()\n",
        "            break\n",
        "\n",
        "    if eps>0 and eps%eval_eps==0:\n",
        "        # Sucess rate\n",
        "        success_rate = success_count / eval_eps\n",
        "        success_count = 0\n",
        "        # Save the best model\n",
        "        if success_rate >= max_success_rate:\n",
        "            max_success_rate = success_rate\n",
        "            if training:\n",
        "                print(\"Save model to \" + model_path)\n",
        "                agent_mind_sac.save_load_model(\"save\", model_path)\n",
        "        print(\"Success Rate (current/max):\", success_rate, \"/\", max_success_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cp7MhrmT2f2m"
      },
      "source": [
        "visualize(agent_mind_sac, total_eps=4, map_path=terrain, gif_path=gif_path, gif_name=\"sac_\"+str(eps).zfill(4)+\".gif\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X20DniDk2sFX"
      },
      "source": [
        "## PPO Training "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZZJ-7_v2vme"
      },
      "source": [
        "from torch.distributions import Normal\n",
        "\n",
        "class PPOPolicy(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PPOPolicy, self).__init__()\n",
        "        self.layer1 = nn.Linear(23, 512)\n",
        "        self.layer2 = nn.Linear(512, 512)\n",
        "        self.layer3 = nn.Linear(512, 512)\n",
        "        self.fc4_mean = nn.Linear(512, 2)\n",
        "        self.fc4_logstd = nn.Linear(512, 2)\n",
        "\n",
        "    def forward(self, s):\n",
        "        h_fc1 = F.relu(self.layer1(s))\n",
        "        h_fc2 = F.relu(self.layer2(h_fc1))\n",
        "        h_fc3 = F.relu(self.layer3(h_fc2))\n",
        "        a_mean = torch.tanh(self.fc4_mean(h_fc3))\n",
        "        a_logstd = self.fc4_logstd(h_fc3)\n",
        "        return a_mean, a_logstd\n",
        "    \n",
        "    def distribution(self, s):\n",
        "        a_mean, a_logstd = self.forward(s)\n",
        "        a_std = a_logstd.exp()\n",
        "        dist = Normal(a_mean, a_std)\n",
        "        return dist\n",
        "\n",
        "    def sample(self, s):\n",
        "        dist = self.distribution(s)\n",
        "        a_samp = dist.sample()\n",
        "        logp = dist.log_prob(a_samp)\n",
        "        return a_samp, logp\n",
        "\n",
        "class ValueNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ValueNet, self).__init__()\n",
        "        self.layer1 = nn.Linear(23, 512)\n",
        "        self.layer2 = nn.Linear(512, 512)\n",
        "        self.layer3 = nn.Linear(512, 512)\n",
        "        self.layer4 = nn.Linear(512, 1)\n",
        "    \n",
        "    def forward(self, s):\n",
        "        h_fc1 = F.relu(self.layer1(s))\n",
        "        h_fc2 = F.relu(self.layer2(h_fc1))\n",
        "        h_fc3 = F.relu(self.layer3(h_fc2))\n",
        "        v_out = self.layer4(h_fc3)\n",
        "        return v_out\n",
        "\n",
        "agent_mind_ppo = PPO(\n",
        "    model = [PPOPolicy, ValueNet],\n",
        "    learning_rate = [0.0001, 0.0001],\n",
        "    reward_decay = 0.99,\n",
        "    batch_size = 10)\n",
        "\n",
        "render = True\n",
        "if load_model:\n",
        "    print(\"Load model ...\", model_path)\n",
        "    agent_mind_ppo.save_load_model(\"load\", model_path)\n",
        "\n",
        "\n",
        "env = IndoorDeepRL(map_path=terrain)\n",
        "total_step = 0\n",
        "max_success_rate = 0\n",
        "success_count = 0\n",
        "\n",
        "print(\"eps, step, total_step, action[0], reward, acc_reward/step\")\n",
        "for eps in range(4500):\n",
        "    state = env.createInstance()\n",
        "    step = 0\n",
        "    loss_a = loss_c = 0.\n",
        "    acc_reward = 0.\n",
        "\n",
        "    while(True):\n",
        "        # Choose action and run\n",
        "        if training:\n",
        "            action, logp = agent_mind_ppo.choose_action(state, eval=False)\n",
        "        else:\n",
        "            action, logp = agent_mind_ppo.choose_action(state, eval=True)\n",
        "        state_next, reward, terminated = env.step(action)\n",
        "        end = 0 if terminated else 1\n",
        "        agent_mind_ppo.store_transition(state, action, reward, state_next, end, logp)\n",
        "\n",
        "        # Render environment\n",
        "        im = env.render(gui=render)\n",
        "\n",
        "        # Learn the model\n",
        "        step += 1\n",
        "        total_step += 1\n",
        "\n",
        "        # Print information\n",
        "        acc_reward += reward\n",
        "        print('\\r {:3d}; {:4d}; {:6d}; {:+.2f}; {:+.2f}; {:.2f} '\\\n",
        "                .format(eps, step, total_step, action[0], reward, acc_reward/step), end='')\n",
        "\n",
        "        state = state_next.copy()\n",
        "        if terminated or step>200:\n",
        "            # Count the successful times\n",
        "            if reward > 5:\n",
        "                success_count += 1\n",
        "            print()\n",
        "            break\n",
        "\n",
        "    if agent_mind_ppo.memory_counter >= agent_mind_ppo.batch_size:\n",
        "        agent_mind_ppo.learn(100)\n",
        "\n",
        "    if eps>0 and eps%eval_eps==0:\n",
        "        # Sucess rate\n",
        "        success_rate = success_count / eval_eps\n",
        "        success_count = 0\n",
        "        # Save the best model\n",
        "        if success_rate >= max_success_rate:\n",
        "            max_success_rate = success_rate\n",
        "            if training:\n",
        "                print(\"Save model to \" + model_path)\n",
        "                agent_mind_ppo.save_load_model(\"save\", model_path)\n",
        "        print(\"Success Rate (current/max):\", success_rate, \"/\", max_success_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJHuV6aj2xd1"
      },
      "source": [
        "visualize(agent_mind_ppo, total_eps=4, map_path=terrain, gif_path=gif_path, gif_name=\"ppo_\"+str(eps).zfill(4)+\".gif\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}