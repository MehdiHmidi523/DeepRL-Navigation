{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDPG, PPO, SAC under Partial Observability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from utils import SparseDepth\n",
    "from vision import Vision\n",
    "from agentKinematics import RoboticAssistant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class IndoorDeepRL:\n",
    "    def __init__(self, map_path=\"complex.png\"):\n",
    "        self.terra = cv2.flip(cv2.imread(map_path), 0)\n",
    "        self.terra[self.terra > 128] = 255\n",
    "        self.terra[self.terra <= 128] = 0\n",
    "        self.m = np.asarray(self.terra)\n",
    "        self.m = cv2.cvtColor(self.m, cv2.COLOR_RGB2GRAY)\n",
    "        self.m = self.m.astype(float) / 255.\n",
    "        self.terra = self.terra.astype(float) / 255.\n",
    "        self.lmodel = Vision(self.m)\n",
    "\n",
    "    def createInstance(self):\n",
    "        self.robot = RoboticAssistant(d=5, wu=9, wv=4, car_w=9, car_f=7, car_r=10, dt=0.1)\n",
    "        self.robot.x, self.robot.y = self.random_start_travesable()\n",
    "        self.robot.theta = 360 * np.random.random()\n",
    "        self.pos = (self.robot.x, self.robot.y, self.robot.theta)\n",
    "\n",
    "        self.target = self.random_start_travesable()\n",
    "        self.target_euclidian = np.sqrt((self.robot.x - self.target[0]) ** 2 + (self.robot.y - self.target[1]) ** 2)\n",
    "        target_angle = np.arctan2(self.target[1] - self.robot.y, self.target[0] - self.robot.x) - np.deg2rad(self.robot.theta)\n",
    "        target_distance = [self.target_euclidian * np.cos(target_angle), self.target_euclidian * np.sin(target_angle)]\n",
    "\n",
    "        self.sdata, self.plist = self.lmodel.measure_depth(self.pos)\n",
    "        state = self.existance(self.sdata, target_distance)\n",
    "        return state\n",
    "\n",
    "    def step(self, action):\n",
    "        self.robot.control((action[0] + 1) / 2 * self.robot.v_interval, action[1] * self.robot.w_interval)\n",
    "        self.robot.update()\n",
    "\n",
    "        e1,e2,e3,e4 = self.robot.dimensions\n",
    "        ee1 = SparseDepth(e1[0], e2[0], e1[1], e2[1])\n",
    "        ee2 = SparseDepth(e1[0], e3[0], e1[1], e3[1])\n",
    "        ee3 = SparseDepth(e3[0], e4[0], e3[1], e4[1])\n",
    "        ee4 = SparseDepth(e4[0], e2[0], e4[1], e2[1])\n",
    "        check = ee1+ee2+ee3+ee4\n",
    "        \n",
    "        collision = False\n",
    "        for points in check:\n",
    "            if self.m[int(points[1]),int(points[0])]<0.5:\n",
    "                collision = True\n",
    "                self.robot.redo()\n",
    "                self.robot.velocity = -0.5 * self.robot.velocity\n",
    "                break\n",
    "        \n",
    "        self.pos = (self.robot.x, self.robot.y, self.robot.theta)\n",
    "        self.sdata, self.plist = self.lmodel.measure_depth(self.pos)\n",
    "\n",
    "        action_r = 0.05 if action[0] < -0.5 else 0\n",
    "        \n",
    "        curr_target_dist = np.sqrt((self.robot.x - self.target[0]) ** 2 + (self.robot.y - self.target[1]) ** 2)\n",
    "        distance_reward = self.target_euclidian - curr_target_dist\n",
    "   \n",
    "        s_orien = np.rad2deg(np.arctan2(self.target[1] - self.robot.y, self.target[0] - self.robot.x))\n",
    "        orientation_error = (s_orien - self.robot.theta) % 360\n",
    "        if orientation_error > 180:\n",
    "            orientation_error = 360 - orientation_error\n",
    "        orientation_reward = np.deg2rad(orientation_error)\n",
    "  \n",
    "        reward = distance_reward - orientation_reward - 0.6 * action_r\n",
    "        \n",
    "        terminated = False\n",
    "        \n",
    "        if curr_target_dist < 20:\n",
    "            reward = 20\n",
    "            terminated = True\n",
    "        if collision:\n",
    "            reward = -15\n",
    "            terminated = True\n",
    "        \n",
    "        self.target_euclidian = curr_target_dist\n",
    "        target_angle = np.arctan2(self.target[1] - self.robot.y, self.target[0] - self.robot.x) - np.deg2rad(self.robot.theta)\n",
    "        target_distance = [self.target_euclidian * np.cos(target_angle), self.target_euclidian * np.sin(target_angle)]\n",
    "        state_next = self.existance(self.sdata, target_distance)\n",
    "\n",
    "        return state_next, reward, terminated\n",
    "\n",
    "    def render(self, gui=True):\n",
    "        experiment_space = self.terra.copy()\n",
    "        for pts in self.plist:\n",
    "            cv2.line(\n",
    "                experiment_space,\n",
    "                (int(1*self.pos[0]), int(1*self.pos[1])), \n",
    "                (int(1*pts[0]), int(1*pts[1])),\n",
    "                (0.0,1.0,0.0), 1)\n",
    "\n",
    "        cv2.circle(experiment_space, (int(1*self.target[0]), int(1*self.target[1])), 10, (1.0,0.5,0.7), 3)\n",
    "        experiment_space = self.robot.render(experiment_space)\n",
    "        experiment_space = cv2.flip(experiment_space,0)\n",
    "        if gui:\n",
    "            cv2.imshow(\"Mapless Navigation\",experiment_space)\n",
    "            k = cv2.waitKey(1)\n",
    "\n",
    "        return experiment_space.copy()\n",
    "    \n",
    "    def random_start_travesable(self):\n",
    "        height, width = self.m.shape[0], self.m.shape[1]\n",
    "        tx = np.random.randint(0,width)\n",
    "        ty = np.random.randint(0,height)\n",
    "\n",
    "        kernel = np.ones((10,10),np.uint8)  \n",
    "        m_dilate = 1-cv2.dilate(1-self.m, kernel, iterations=3)\n",
    "        while(m_dilate[ty, tx] < 0.5):\n",
    "            tx = np.random.randint(0,width)\n",
    "            ty = np.random.randint(0,height)\n",
    "        return tx, ty\n",
    "    \n",
    "    def existance(self, sensor, target):\n",
    "        si = [s/200 for s in sensor]\n",
    "        ti = [t/500 for t in target]\n",
    "        return si + ti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainning and Path configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training = True\n",
    "render = True\n",
    "load_model = False\n",
    "terrain = \"map.png\"\n",
    "gif_path = \"performance/\"\n",
    "model_path = \"models/\"\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Episode Visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def performance_measure(episode, agent, s_count, max_rate):\n",
    "     if episode>0 and episode%50==0:\n",
    "        s_rate = s_count / 50\n",
    "        if s_rate >= max_rate:\n",
    "            max_rate = s_rate\n",
    "            if training:\n",
    "                print(\"Save model to \" + model_path)\n",
    "                agent.save_load_model(\"save\", model_path)\n",
    "        print(\"Success Rate (current/max):\", s_rate, \"/\", max_rate)\n",
    "     return max_rate\n",
    "\n",
    "def visualize(agent, total_eps=2, message=False, render=False, map_path=\"large.png\", gif_path=\"performance/\", gif_name=\"test.gif\"):\n",
    "    if not os.path.exists(gif_path):\n",
    "        os.makedirs(gif_path)\n",
    "\n",
    "    images = []\n",
    "\n",
    "    mother_nature = IndoorDeepRL(map_path=terrain)\n",
    "    for eps in range(total_eps):\n",
    "        step = 0\n",
    "        max_success_rate = 0\n",
    "        success_count = 0\n",
    "\n",
    "        state = mother_nature.createInstance()\n",
    "        r_eps = []\n",
    "        acc_reward = 0.\n",
    "\n",
    "        while(True):\n",
    "            action = agent.choose_action(state, eval=True)\n",
    "            state_next, reward, terminated = mother_nature.step(action)\n",
    "            displayed = mother_nature.render(gui=render)\n",
    "            im_pil = Image.fromarray(cv2.cvtColor(np.uint8(displayed*255),cv2.COLOR_BGR2RGB))\n",
    "            images.append(im_pil)\n",
    "            r_eps.append(reward)\n",
    "            acc_reward += reward\n",
    "            \n",
    "            if message:\n",
    "                print('\\rEps: {:2d}| Step: {:4d} | action:{:+.2f}| R:{:+.2f}| Reps:{:.2f}  '\\\n",
    "                        .format(eps, step, action[0], reward, acc_reward), end='')\n",
    "            \n",
    "            state = state_next.copy()\n",
    "            step += 1\n",
    "            \n",
    "            if terminated or step>200:\n",
    "                if message:\n",
    "                    print()\n",
    "                break\n",
    "\n",
    "    print(\"Create GIF ...\")\n",
    "    if gif_path is not None:\n",
    "        images[0].save(gif_path+gif_name, save_all=True, append_images=images[1:], optimize=True, duration=40, loop=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDPG Agent training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import ddpg\n",
    "\n",
    "batch_size = 64\n",
    "LOG_SIG_MAX = 2\n",
    "LOG_SIG_MIN = -20\n",
    "epsilon = 1e-6\n",
    "\n",
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.layer1 = nn.Linear(23, 512)\n",
    "        self.layer2 = nn.Linear(512, 512)\n",
    "        self.layer3 = nn.Linear(512, 512)\n",
    "        self.layer4 = nn.Linear(512, 2)\n",
    "\n",
    "    def forward(self, s):\n",
    "        hidden_layer_1 = F.relu(self.layer1(s))\n",
    "        hidden_layer_2 = F.relu(self.layer2(hidden_layer_1))\n",
    "        hidden_layer_3 = F.relu(self.layer3(hidden_layer_2))\n",
    "        return torch.tanh(self.layer4(hidden_layer_3)) # one mu\n",
    "\n",
    "class QNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(QNet, self).__init__()\n",
    "        self.layer1 = nn.Linear(23, 512)\n",
    "        self.layer2 = nn.Linear(512+2, 512)\n",
    "        self.layer3 = nn.Linear(512, 512)\n",
    "        self.layer4 = nn.Linear(512, 1)\n",
    "    \n",
    "    def forward(self, s, a):\n",
    "        hidden_layer_1 = F.relu(self.layer1(s))\n",
    "        hidden_layer_1_a = torch.cat((hidden_layer_1, a), 1)\n",
    "        hidden_layer_2 = F.relu(self.layer2(hidden_layer_1_a))\n",
    "        hidden_layer_3 = F.relu(self.layer3(hidden_layer_2))\n",
    "        return self.layer4(hidden_layer_3)\n",
    "\n",
    "\n",
    "agent_mind_ddpg = ddpg.DDPG(base_net = [PolicyNet, QNet], b_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eps, step, total_steps, action[0], reward, loss_a, loss_c, agent_mind_ddpg.epsilon, acc_reward/step\n",
      "  0 ;   49;     49; +0.17; -15.00; +0.00; +0.00; 1.000; 1.64\n",
      "0\n",
      "  1 ;   16;     65; +0.39; -1.01; +0.00; +0.00; 1.000; -0.40"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([64, 64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1 ;  150;    199; -1.00; -0.76; +1.48; +19.20; 0.999; -2.20"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-6-86672bd2e69d>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     24\u001B[0m         \u001B[0magent_mind_ddpg\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstore_transition\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstate\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maction\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreward\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstate_next\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mend\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     25\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 26\u001B[0;31m         \u001B[0mdisplayed\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmother_nature\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrender\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mgui\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mrender\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     27\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     28\u001B[0m         \u001B[0mloss_a\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mloss_c\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m0.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-2-6cc6e48ac8d1>\u001B[0m in \u001B[0;36mrender\u001B[0;34m(self, gui)\u001B[0m\n\u001B[1;32m     90\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mgui\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     91\u001B[0m             \u001B[0mcv2\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mimshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Mapless Navigation\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mexperiment_space\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 92\u001B[0;31m             \u001B[0mk\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcv2\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwaitKey\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     93\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     94\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mexperiment_space\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcopy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "if load_model:\n",
    "    print(\"Load model ...\", model_path)\n",
    "    agent_mind_ddpg.save_load_model(\"load\", model_path)\n",
    "\n",
    "mother_nature = IndoorDeepRL(map_path=terrain)\n",
    "total_steps = 0\n",
    "max_success_rate = 0\n",
    "success_count = 0\n",
    "print(\"eps, step, total_steps, action[0], reward, loss_a, loss_c, agent_mind_ddpg.epsilon, acc_reward/step\")\n",
    "\n",
    "for eps in range(4500):\n",
    "    state = mother_nature.createInstance()\n",
    "    step = 0\n",
    "    loss_a = loss_c = 0.\n",
    "    acc_reward = 0.\n",
    "    while True:\n",
    "        if training:\n",
    "            action = agent_mind_ddpg.choose_action(state, eval=False)\n",
    "        else:\n",
    "            action = agent_mind_ddpg.choose_action(state, eval=True)\n",
    "        \n",
    "        state_next, reward, terminated = mother_nature.step(action)\n",
    "        end = 0 if terminated else 1\n",
    "        agent_mind_ddpg.store_transition(state, action, reward, state_next, end)\n",
    "        \n",
    "        displayed = mother_nature.render(gui=render)\n",
    "\n",
    "        loss_a = loss_c = 0.\n",
    "        if total_steps > batch_size and training:\n",
    "            loss_a, loss_c = agent_mind_ddpg.learn()\n",
    "        step += 1\n",
    "        total_steps += 1\n",
    "\n",
    "        acc_reward += reward\n",
    "        print('\\r{:3d} ; {:4d}; {:6d}; {:+.2f}; {:+.2f}; {:+.2f}; {:+.2f}; {:.3f}; {:.2f}'\n",
    "              .format(eps, step, total_steps, action[0], reward, loss_a, loss_c, agent_mind_ddpg._e, acc_reward/step), end='')\n",
    "        state = state_next.copy()\n",
    "        \n",
    "        if terminated or step>200:\n",
    "            if reward > 5:\n",
    "                success_count += 1\n",
    "            print()\n",
    "            break\n",
    "\n",
    "    max_success_rate = performance_measure(eps, agent_mind_ddpg, success_count, max_success_rate)\n",
    "    print(max_success_rate)\n",
    "    success_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "visualize(agent_mind_ddpg, total_eps=8, map_path=terrain, gif_path=gif_path, gif_name=\"DDPG_\"+str(eps).zfill(4)+\".gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "import sac\n",
    "\n",
    "batch_size = 64\n",
    "LOG_SIG_MAX = 2\n",
    "LOG_SIG_MIN = -20\n",
    "epsilon = 1e-6\n",
    "\n",
    "\n",
    "class PolicyNetGaussian(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PolicyNetGaussian, self).__init__()\n",
    "        self.layer1 = nn.Linear(23, 512)\n",
    "        self.layer2 = nn.Linear(512, 512)\n",
    "        self.layer3 = nn.Linear(512, 512)\n",
    "        self.layer_4_mean = nn.Linear(512, 2)\n",
    "        self.layer_4_standard_log = nn.Linear(512, 2)\n",
    "\n",
    "    def forward(self, s):\n",
    "        hidden_layer_1 = F.relu(self.layer1(s))\n",
    "        hidden_layer_2 = F.relu(self.layer2(hidden_layer_1))\n",
    "        hidden_layer_3 = F.relu(self.layer3(hidden_layer_2))\n",
    "        return self.layer_4_mean(hidden_layer_3), torch.clamp(self.layer_4_standard_log(hidden_layer_3), min=LOG_SIG_MIN, max=LOG_SIG_MAX)\n",
    "    \n",
    "    def sample(self, s):\n",
    "        a_mean, standard_log = self.forward(s)\n",
    "        a_std = standard_log.exp()\n",
    "        flow = Normal(a_mean, a_std)\n",
    "        position_x = flow.rsample()\n",
    "        A_ = torch.tanh(position_x)\n",
    "        log_prob = flow.log_prob(position_x) -  torch.log(1 - A_.pow(2) + epsilon)\n",
    "        return A_, log_prob.sum(1, keepdim=True), torch.tanh(a_mean)\n",
    "\n",
    "agent_mind_sac = sac.SAC( model = [PolicyNetGaussian, QNet], b_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "if load_model:\n",
    "    print(\"Load model ...\", model_path)\n",
    "    agent_mind_sac.save_load_model(\"load\", model_path)\n",
    "\n",
    "mother_nature = IndoorDeepRL(map_path=terrain)\n",
    "total_step = 0\n",
    "max_success_rate = 0\n",
    "success_count = 0\n",
    "print(\"eps, step, total_step, action[0], reward, loss_a, loss_c, agent_mind_sac.alpha, acc_reward/step\")\n",
    "\n",
    "for eps in range(4500):\n",
    "    state = mother_nature.createInstance()\n",
    "    step = 0\n",
    "    loss_a = loss_c = 0.\n",
    "    acc_reward = 0.\n",
    "\n",
    "    while True:\n",
    "        if training:\n",
    "            action = agent_mind_sac.choose_action(state, eval=False)\n",
    "        else:\n",
    "            action = agent_mind_sac.choose_action(state, eval=True)\n",
    "        \n",
    "        state_next, reward, terminated = mother_nature.step(action)\n",
    "        end = 0 if terminated else 1\n",
    "        agent_mind_sac.store_transition(state, action, reward, state_next, end)\n",
    "\n",
    "        displayed = mother_nature.render(gui=render)\n",
    "\n",
    "        loss_a = loss_c = 0.\n",
    "        if total_step > batch_size and training:\n",
    "            loss_a, loss_c = agent_mind_sac.learn()\n",
    "\n",
    "        step += 1\n",
    "        total_step += 1\n",
    "        acc_reward += reward\n",
    "\n",
    "        print('\\r {:3d}; {:4d}; {:6d}; {:+.2f}; {:+.2f}; {:+.2f}; {:+.2f};  {:.3f}; {:.2f}  '\\\n",
    "                .format(eps, step, total_step, action[0], reward, loss_a, loss_c, agent_mind_sac.alpha, acc_reward/step), end='')\n",
    "\n",
    "        state = state_next.copy()\n",
    "        if terminated or step>200:\n",
    "            if reward > 5:\n",
    "                success_count += 1\n",
    "            print()\n",
    "            break\n",
    "\n",
    "    max_success_rate = performance_measure(eps, agent_mind_sac, success_count, max_success_rate)\n",
    "    success_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "visualize(agent_mind_sac, total_eps=8, map_path=terrain, gif_path=gif_path, gif_name=\"SAC_\"+str(eps).zfill(4)+\".gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO Agent training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import ppo\n",
    "\n",
    "\n",
    "class PPOPolicy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PPOPolicy, self).__init__()\n",
    "        self.layer1 = nn.Linear(23, 512)\n",
    "        self.layer2 = nn.Linear(512, 512)\n",
    "        self.layer3 = nn.Linear(512, 512)\n",
    "        self.layer_4_mean = nn.Linear(512, 2)\n",
    "        self.layer_4_standard_log = nn.Linear(512, 2)\n",
    "\n",
    "    def forward(self, s):\n",
    "        hidden_layer_1 = F.relu(self.layer1(s))\n",
    "        hidden_layer_2 = F.relu(self.layer2(hidden_layer_1))\n",
    "        hidden_layer_3 = F.relu(self.layer3(hidden_layer_2))\n",
    "        return  torch.tanh(self.layer_4_mean(hidden_layer_3)), self.layer_4_standard_log(hidden_layer_3)\n",
    "    \n",
    "    def distribution(self, s):\n",
    "        a_mean, standard_log = self.forward(s)\n",
    "        return Normal(a_mean, standard_log.exp())\n",
    "\n",
    "    def sample(self, s):\n",
    "        flow = self.distribution(s)\n",
    "        a_samp = flow.sample()\n",
    "        return a_samp, flow.log_prob(a_samp)\n",
    "\n",
    "class ValueNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ValueNet, self).__init__()\n",
    "        self.layer1 = nn.Linear(23, 512)\n",
    "        self.layer2 = nn.Linear(512, 512)\n",
    "        self.layer3 = nn.Linear(512, 512)\n",
    "        self.layer4 = nn.Linear(512, 1)\n",
    "    \n",
    "    def forward(self, s):\n",
    "        hidden_layer_1 = F.relu(self.layer1(s))\n",
    "        hidden_layer_2 = F.relu(self.layer2(hidden_layer_1))\n",
    "        hidden_layer_3 = F.relu(self.layer3(hidden_layer_2))\n",
    "        return self.layer4(hidden_layer_3)\n",
    "\n",
    "agent_mind_ppo = ppo.PPO(model = [PPOPolicy, ValueNet], batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.distributions import Normal\n",
    "\n",
    "render = True\n",
    "if load_model:\n",
    "    print(\"Load model ...\", model_path)\n",
    "    agent_mind_ppo.save_load_model(\"load\", model_path)\n",
    "\n",
    "\n",
    "mother_nature = IndoorDeepRL(map_path=terrain)\n",
    "total_step = 0\n",
    "max_success_rate = 0\n",
    "success_count = 0\n",
    "print(\"eps, step, total_step, action[0], reward, acc_reward/step\")\n",
    "\n",
    "for eps in range(4500):\n",
    "    state = mother_nature.createInstance()\n",
    "    step = 0\n",
    "    loss_a = loss_c = 0.\n",
    "    acc_reward = 0.\n",
    "\n",
    "    while True:\n",
    "        if training:\n",
    "            action, policy_log = agent_mind_ppo.choose_action(state, eval=False)\n",
    "        else:\n",
    "            action, policy_log = agent_mind_ppo.choose_action(state, eval=True)\n",
    "        \n",
    "        state_next, reward, terminated = mother_nature.step(action)\n",
    "\n",
    "        if terminated:\n",
    "            end = 0\n",
    "        else:\n",
    "            end = 1\n",
    "\n",
    "        agent_mind_ppo.store_transition(state, action, reward, state_next, end, policy_log)\n",
    "        displayed = mother_nature.render(gui=render)\n",
    "\n",
    "        step += 1\n",
    "        total_step += 1\n",
    "\n",
    "        acc_reward += reward\n",
    "        print('\\r {:3d}; {:4d}; {:6d}; {:+.2f}; {:+.2f}; {:.2f} '\\\n",
    "                .format(eps, step, total_step, action[0], reward, acc_reward/step), end='')\n",
    "\n",
    "        state = state_next.copy()\n",
    "        if terminated or step>200:\n",
    "            if reward > 5:\n",
    "                success_count += 1\n",
    "            print()\n",
    "            break\n",
    "\n",
    "    agent_mind_ppo.learn()\n",
    "\n",
    "    max_success_rate = performance_measure(eps, agent_mind_ddpg, success_count, max_success_rate)\n",
    "    success_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "visualize(agent_mind_ppo, total_eps=4, map_path=terrain, gif_path=gif_path, gif_name=\"PPO_\"+str(eps).zfill(4)+\".gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}